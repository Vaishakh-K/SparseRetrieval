# UniCOIL
This page describes how to train the uniCOIL model in the following paper:

> Jimmy Lin and Xueguang Ma. [A Few Brief Notes on DeepImpact, COIL, and a Conceptual Framework for Information Retrieval Techniques.](https://arxiv.org/abs/2106.14807) _arXiv:2106.14807_.

UniCOIL is developed based on COIL, most of the process are same with the guidance in main page.

For UniCOIL, we focus on MS MARCO Passage ranking task.

## TL;DR
UniCOIL has three main differences from COIL:
1. UniCOIL restrict the `tok_dim=1`, i.e. generate weights for each token rather than vector representation
2. Use new terms in generated query from doc2query to expand passage
3. UniCOIL can be indexed and search using traditional inverted index like Anserini(Lucene)

## Doc2Query
For doc2query generation, please see [docTTTTTquery](https://github.com/castorini/docTTTTTquery)
repo for details

## Resource
- `psg-train-d2q` : train data with d2q expanded terms appended, [download](https://www.dropbox.com/s/j1vp1nixn3n2yv0/psg-train-d2q.tar.gz?dl=0)
- `corpus-d2q`    : tokenized corpus with d2q expanded terms appended [download](https://www.dropbox.com/s/aw8qwqlsbtzxcbz/corpus_d2q.tar.gz?dl=0)
- `msmarco-passage-d2q`: raw collection of msmarco-passage with doc2query expansion [download](https://www.dropbox.com/s/2ikdxw9hbom9boj/msmarco-passage-expanded.tar.gz?dl=0)


## Training

```bash
TRAIN=psg-train-d2q
python run_marco.py \
  --output_dir model_unicoil \
  --model_name_or_path bert-base-uncased \
  --do_train \
  --save_steps 4000 \
  --train_dir ${TRAIN} \
  --q_max_len 16 \
  --p_max_len 192 \
  --fp16 \
  --per_device_train_batch_size 8 \
  --train_group_size 8 \
  --no_cls \
  --token_dim 1 \
  --token_rep_relu \
  --warmup_ratio 0.1 \
  --learning_rate 5e-6 \
  --num_train_epochs 5 \
  --overwrite_output_dir \
  --dataloader_num_workers 16 \
  --no_sep \
  --pooling max
```

## Encoding
```bash
ENCODE_OUT_DIR=embeddings_unicoil
CKPT_DIR=model_unicoil
CORPUS=corpus-d2q
for i in $(seq -f "%02g" 0 99)
do
  mkdir ${ENCODE_OUT_DIR}/split${i}
  python run_marco.py \
    --output_dir $ENCODE_OUT_DIR \
    --model_name_or_path $CKPT_DIR \
    --tokenizer_name bert-base-uncased \
    --token_dim 1 \
    --no_cls \
    --do_encode \
    --no_sep \
    --p_max_len 192 \
    --pooling max \
    --fp16 \
    --per_device_eval_batch_size 128 \
    --dataloader_num_workers 12 \
    --encode_in_path ${CORPUS}/split${i} \
    --encoded_save_path ${ENCODE_OUT_DIR}/split${i}
done
```

```bash
ENCODE_QRY_OUT_DIR=query_embeddings_unicoil
CKPT_DIR=model_unicoil
QUERY=queries.dev.small.json
python run_marco.py \
  --output_dir $ENCODE_QRY_OUT_DIR \
  --model_name_or_path $CKPT_DIR \
  --tokenizer_name bert-base-uncased \
  --token_dim 1 \
  --no_cls \
  --do_encode \
  --p_max_len 64 \
  --fp16 \
  --no_sep \
  --pooling max \
  --per_device_eval_batch_size 128 \
  --dataloader_num_workers 12 \
  --encode_in_path ${QUERY} \
  --encoded_save_path $ENCODE_QRY_OUT_DIR
```

## Convert Embeddings to Anserini Input
Since the weights of each token generated by the model are `float`, to make it able to fit in traditional 
inverted index, we need to convert the weights to `int`.

During our training, we observed that all the weights for query and docs are in the `range(0, 5)`
We do quantization to map the float weigts to integers in `range(0, 2^8)` 
```bash
python doc_emb_to_jsonl.py --input embeddings_unicoil \
                           --output msmarco_passage_unicoil_encoded \
                           --range 5 --quantization 8
```

```bash
python query_emb_to_tsv.py --input query_embeddings_unicoil \
                           --output topics_msmarco_passage_dev_unicol_encoded.tsv \
                           --range 5 --quantization 8
```

## Indexing and Search via Anserini
For indexing, search, and evaluation, 
please see [Anserini](https://github.com/castorini/anserini/blob/master/docs/experiments-msmarco-passage-unicoil.md) repo for details.
